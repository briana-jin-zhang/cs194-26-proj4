<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Facial Keypoint Detection with Neural Networks</title>
    <link href="https://fonts.googleapis.com/css2?family=Kaushan+Script&display=swap" rel="stylesheet">
    <link href="styles/style.css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <h1>Facial Keypoint Detection with Neural Networks</h1>
    <h2>Nose Tip Detection</h2>
    <p>
        First, I used a very small portion of the dataset to predict the nose tip only. The images 
        were compressed to 60x80 pixels and greyscale.
    </p>
    <h3>
        Groundtruth Keypoints
    </h3>
    <div class = 'container'>
        <div class='imgContainer'> 
            <img src="images/nosetip1.png" width=300>
        </div>

        <div class='imgContainer'> 
            <img src="images/nosetip2.png" width=300>
        </div>

        <div class='imgContainer'> 
            <img src="images/nosetip3.png" width=300>
        </div>
    </div>
    <h2>Hyperparameters and Architecture</h2>
    <p>
        I used three convolutional layers, first one of size 5, last two of size 3. The 
        number of output channels started from 1 (since the input was in greyscale) to 16,
        then to 32, then to 64. Then, I had two fully connected layers, first one with output of 64,
        second one with output of 2 since we wanted a single (x, y) point for the nose.
        
        The convolutional layers each had relu and maxpooling applied, whereas the first fully connected 
        layer had relu applied. 

        I used an Adam optimizer and tried an initial learning rate of 1e-2, 1e-3, and 1e-4, finding that 1e-3
        was the best out of the three. I ran for 15 epochs. 
    </p>
    <h3>
        MSE Loss
    </h3>
    <div class = 'container'>
        <div class='imgContainer'> 
            <img src="images/noseloss.png" width=500>
        </div>
    </div>
    <h3>
        Sampled Outputs 
    </h3>
    <div class = 'container'>
        <div class='imgContainer'> 
            <img src="images/noseout1.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/noseout2.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/noseout3.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/noseout4.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/noseout5.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/noseout6.png" width=300>
        </div>
    </div>
    <h3>
        Best and Worst Cases 
    </h3>
    <div class = 'container'>
        <div class='imgContainer'> 
            <img src="images/bestnose1.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/bestnose2.png" width=300>
        </div>
    </div>
    <div class = 'container'>
        <div class='imgContainer'> 
            <img src="images/worstnose1.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/worstnose2.png" width=300>
        </div>
    </div>
    <p>
        These images were picked based off of highest and lowest MSE loss.
        I believe the reason why it fails in the third picture might be because the mustache confused the network, 
        and the reason why it fails in the fourth picture is because her angle is a bit unusual, compared to the other straight forward faces, like the photo that performs the best.
    </p>


    <h2>Full Facial Keypoints Detection</h2>
    <p>
        Next, I repeat the process with the same training and validation data, this time with all the facial keypoints.
        The data was augmented with random rotations and crops. 
    </p>

    <h3>
        Groundtruth Keypoints
    </h3>
    <div class = 'container'>
        <div class='imgContainer'> 
            <img src="images/fface1.png" width=300>
        </div>

        <div class='imgContainer'> 
            <img src="images/fface2.png" width=300>
        </div>

        <div class='imgContainer'> 
            <img src="images/fface3.png" width=300>
        </div>
    </div>
    <h2>Hyperparameters and Architecture</h2>
    <p>
        I used five convolutional layers, first one of size 5, the rest of size 3. The 
        number of output channels started from 1 again to 16, and continuously doubled. 
        Then, I had two fully connected layers, first one with output of 256 this time,
        since there were more convolutional layers and following the powers of two, and
        second one with output of 58 * 2 since we wanted a single 58 points this time.
        
        Again, the convolutional layers each had relu and maxpooling applied, whereas the 
        first fully connected layer had relu applied. 

        Again I used an Adam optimizer and tried an initial learning rate of 1e-2, 1e-3, and 1e-4, 
        finding that 1e-3 was the best out of the three. I ran for 20 epochs this time since there 
        were more points, though I could've ran for longer. 
    </p>
    <h3>
        MSE Loss
    </h3>
    <div class = 'container'>
        <div class='imgContainer'> 
            <img src="images/ffaceloss.png" width=500>
        </div>
    </div>
    <h3>
        Sampled Outputs 
    </h3>
    <div class = 'container'>
        <div class='imgContainer'> 
            <img src="images/ffaceout1.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/ffaceout2.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/ffaceout3.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/ffaceout4.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/ffaceout5.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/ffaceout6.png" width=300>
        </div>
    </div>
    <h3>
        Best and Worst Cases 
    </h3>
    <div class = 'container'>
        <div class='imgContainer'> 
            <img src="images/bestfface1.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/bestfface2.png" width=300>
        </div>
    </div>
    <div class = 'container'>
        <div class='imgContainer'> 
            <img src="images/worstfface1.png" width=300>
        </div>
        <div class='imgContainer'> 
            <img src="images/worstfface2.png" width=300>
        </div>
    </div>

    <p>
        Very interestingly, the second best face was also the second worst face for nose detection.
        I suspect the reason for the failures might have been how the person is looking more downwards
        rather than straight forwards. It seems the model did well on face that were looking straight 
        on or to the side. As a reference, one of the samples shown above corresponds to the worst case 
        for nose detection, which also did not do very well.
    </p>

    <h3>
        Learned Filters 
    </h3>
    <p>
        These were the learned filters of the first layer. In some of them, you can see a sense of a 
        round shape edge being learned.
    </p>
    <div class = 'container'>
        <div class='imgContainer'>
            <img src="images/filters.png" width=800>
        </div>
    </div>

    <h2>
        Train With Larger Dataset
    </h2>

    <p>
        The process was repeated with a much larger dataset (6666 images instead of 240).
        This data had bounding boxes so it did not make sense to crop. Instead, I just 
        augmented the data with rotations.
    </p>

    <h2>Hyperparameters and Architecture</h2>

    <p>
        I used a resnet18 model, changing the first conv net with 1 input channel instead 
        of three since it was grayscale, and the last channel to have output of 68*2 since 
        there are now 68 labeled points. I tried a few hyperparameters, running for 80 epochs.
        I found the best was having a batch size of 32 and learning rate of 1e-2. I tried a few 
        times experimentally at the beginning before graphs with 1e-2 and larger batch sizes,
        but did not do much better than 0.0003.
    </p>


    <div class = 'container'>
        <div class='imgContainer'>
            <ul> 
                <li>batch size: 32</li>
                <li>learning rate: 1e-2</li>
                <li>min validation error: 0.00029225</li>
                <li>min validation error epoch: 76</li>
            </ul>
            <img src="images/32_2.png" width=300>
        </div>

        <div class='imgContainer'>
            <ul> 
                <li>batch size: 32</li>
                <li>learning rate: 1e-4</li>
                <li>min validation error: 0.000415</li>
                <li>min validation error epoch: 75</li>
            </ul>
            <img src="images/32_4.png" width=300>
        </div>

        <div class='imgContainer'>
            <ul> 
                <li>batch size: 32</li>
                <li>learning rate: 1e-5</li>
                <li>min validation error: 0.0033</li>
                <li>min validation error epoch: 80</li>
            </ul>
            <img src="images/32_5.png" width=300>
        </div>

        <div class='imgContainer'>
            <ul> 
                <li>batch size: 64</li>
                <li>learning rate: 1e-2</li>
                <li>min validation error: 0.00032</li>
                <li>min validation error epoch: 73</li>
            </ul>
            <img src="images/64_2.png" width=300>
        </div>

        <div class='imgContainer'>
            <ul> 
                <li>batch size: 64</li>
                <li>learning rate: 1e-4</li>
                <li>min validation error: 0.0005775</li>
                <li>min validation error epoch: 73</li>
            </ul>
            <img src="images/64_5.png" width=300>
        </div>

        <div class='imgContainer'>
            <ul> 
                <li>batch size: 64</li>
                <li>learning rate: 1e-5</li>
                <li>min validation error: 0.007</li>
                <li>min validation error epoch: 80</li>
            </ul>
            <img src="images/64_5.png" width=300>
        </div>
    </div>

    <h2>Test Set Predictions</h2>

    <p>
        Initially, my models with learning rates of 1e-3 seemed to generate 
        hearts despite the low error of around 0.0003. 
    </p>

    <div class = 'container'>
        <div class='imgContainer'>
            <img src="images/hearts1.png" width=300>
        </div>

        <div class='imgContainer'> 
            <img src="images/hearts2.png" width=300>
        </div>

        <div class='imgContainer'>
            <img src="images/hearts3.png" width=300>
        </div>
    </div>

    <p>
        With hyperparameters of learning rate being 1e-2 and batch size of 32, 
        these were my initial good results for the beginning of the test set. 
    </p>

    <div class = 'container'>
        <div class='imgContainer'>
            <img src="images/test1.png" width=300>
        </div>

        <div class='imgContainer'> 
            <img src="images/test2.png" width=300>
        </div>

        <div class='imgContainer'>
            <img src="images/test3.png" width=300>
        </div>

        <div class='imgContainer'>
            <img src="images/test4.png" width=300>
        </div>

        <div class='imgContainer'> 
            <img src="images/test5.png" width=300>
        </div>

        <div class='imgContainer'>
            <img src="images/test6.png" width=300>
        </div>
    </div>

    <h2>Anti-aliased max pool</h2>

    <p>
        I repeated this with an anti-aliased resnet18 instead of a normal resnet18.
        As can be seen in the graphs below, there was improvement, but not by much.
    </p>

    <div class = 'container'>
        <div class='imgContainer'>
            <ul> 
                <li>batch size: 32</li>
                <li>learning rate: 1e-2</li>
                <li>min validation error: 0.0002574</li>
                <li>min validation error epoch: 76</li>
            </ul>
            <img src="images/anti2.png" width=300>
        </div>

        <div class='imgContainer'>
            <ul> 
                <li>batch size: 32</li>
                <li>learning rate: 1e-3</li>
                <li>min validation error: 0.000267</li>
                <li>min validation error epoch: 73</li>
            </ul>
            <img src="images/anti3.png" width=300>
        </div>

        <div class='imgContainer'>
            <ul> 
                <li>batch size: 32</li>
                <li>learning rate: 1e-4</li>
                <li>min validation error: 0.0003755</li>
                <li>min validation error epoch: 78</li>
            </ul>
            <img src="images/anti4.png" width=300>
        </div>
    </div>

    <div class = 'container'>
        <div class='imgContainer'>
            <img src="images/antitest1.png" width=300>
        </div>

        <div class='imgContainer'> 
            <img src="images/antitest2.png" width=300>
        </div>

        <div class='imgContainer'>
            <img src="images/antitest3.png" width=300>
        </div>

        <div class='imgContainer'>
            <img src="images/antitest4.png" width=300>
        </div>

        <div class='imgContainer'> 
            <img src="images/antitest5.png" width=300>
        </div>

        <div class='imgContainer'>
            <img src="images/antitest6.png" width=300>
        </div>
    </div>

    <h2>Tests outside of Test Set</h2>

    <p>I tried the model on my own personal photos, but the vast majority failed.</p>

    <div class = 'container'>
        <div class='imgContainer'>
            <img src="images/out1.png" width=200>
        </div>

        <div class='imgContainer'> 
            <img src="images/out2.png" width=200>
        </div>

        <div class='imgContainer'>
            <img src="images/out3.png" width=200>
        </div>

        <div class='imgContainer'>
            <img src="images/out4.png" width=200>
        </div>
    </div>

    <p>
        I tested on more photos of myself not shown, and I realized the model kept 
        identifying my eyebrows as eyes, my nose as a mouth, hair for eyebrows, 
        and mouth line as bottom of chin. This was the same for my sister and dad. 
        I then tried the model on the FEI database, thinking perhaps the model 
        possibly did not identify asians well.
    </p>

    <div class = 'container'>
        <div class='imgContainer'>
            <img src="images/fei1.png" width=200>
        </div>

        <div class='imgContainer'> 
            <img src="images/fei2.png" width=200>
        </div>

        <div class='imgContainer'>
            <img src="images/fei3.png" width=200>
        </div>

        <div class='imgContainer'>
            <img src="images/fei4.png" width=200>
        </div>
    </div>

    <h3>Automatic Morphing</h3>

    <p>
        Lastly, since it was able to decently predict the keypoints of the FEI database,
        I used those keypoints to morph together multiple faces automatically by 
        incorporating my previous project.
    </p>

    <img src="images/morph.gif" width=400>

  </body>
</html>
